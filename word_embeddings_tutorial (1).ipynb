{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "colab": {
      "name": "word_embeddings_tutorial.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "qI4di0U1EfgT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class StepLogger():\n",
        "    def __init__(self,capacity):\n",
        "        self.tensor_datas = {}        \n",
        "        self.capacity = capacity\n",
        "        self.added_labels = []\n",
        "        \n",
        "    \n",
        "    def add_info(self,tensor_data,tensor_label):\n",
        "        if tensor_label not in self.added_labels:\n",
        "            self.added_labels.append( tensor_label )\n",
        "        \n",
        "        if tensor_label in self.tensor_datas.keys():\n",
        "            current_arr = self.tensor_datas.get(tensor_label)\n",
        "            if len(current_arr) < self.capacity:\n",
        "                current_arr = self.tensor_datas.get(tensor_label, [])\n",
        "                current_arr.append(tensor_data)\n",
        "        else:\n",
        "            self.tensor_datas[tensor_label] = [tensor_data]\n",
        "    \n",
        "    def get_default_summary(self,show_data=False,summary_count=1):\n",
        "        self.get_summary(self.added_labels,show_data)\n",
        "        \n",
        "    def get_summary(self,labels,show_data=False,summary_count=1):\n",
        "        count = 0\n",
        "        for i in range(summary_count):\n",
        "            for l in labels:\n",
        "                label_data = self.tensor_datas.get(l)[count]\n",
        "                print(l,\" :\")\n",
        "                if torch.is_tensor(label_data):\n",
        "                    print( list(label_data.size() ) )\n",
        "                if not show_data and not torch.is_tensor(label_data):\n",
        "                    print(label_data)\n",
        "                if show_data:    \n",
        "                    print(label_data)"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wo89_YsvuiqZ",
        "colab_type": "text"
      },
      "source": [
        "## **N-Grams**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZmA1oV0q0sxn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "6a27504b-e40b-4e4a-aaa3-34d9c5f60305"
      },
      "source": [
        "word_to_ix = {\"hello\": 0, \"world\": 1}\n",
        "embeds = nn.Embedding(2, 5)  # 2 words in vocab, 5 dimensional embeddings\n",
        "lookup_tensor = torch.tensor([word_to_ix[\"world\"]], dtype=torch.long)\n",
        "hello_embed = embeds(lookup_tensor)\n",
        "print(hello_embed)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[-0.1661, -1.5228,  0.3817, -1.0276, -0.5631]],\n",
            "       grad_fn=<EmbeddingBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K38GWUzi0sxj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "fb24f2ab-f5a4-4b09-fb81-54137bf38d9f"
      },
      "source": [
        "# Author: Robert Guthrie\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "\n",
        "torch.manual_seed(1)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f9652439430>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jXgV3Eq90sxt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 642
        },
        "outputId": "9fe1560b-3883-4d02-f43a-6df299a3ef05"
      },
      "source": [
        "CONTEXT_SIZE = 2\n",
        "EMBEDDING_DIM = 10\n",
        "\n",
        "test_sentence = \"\"\"When forty winters shall besiege thy brow,\n",
        "And dig deep trenches in thy beauty's field,\n",
        "Thy youth's proud livery so gazed on now,\n",
        "Will be a totter'd weed of small worth held:\n",
        "Then being asked, where all thy beauty lies,\n",
        "Where all the treasure of thy lusty days;\n",
        "To say, When forty Summer within thine own deep sunken eyes,\n",
        "Were an all-eating shame, When forty Summer and thriftless praise.\n",
        "How much more praise deserv'd thy beauty's use,\n",
        "If thou couldst answer 'This fair child of mine\n",
        "Shall sum my count, and When forty Summer make my old excuse,'\n",
        "Proving his beauty by succession thine!\n",
        "This were to be new made when thou art old,\n",
        "And When forty Summer see thy blood warm when thou feel'st it cold.\"\"\".split()\n",
        "\n",
        "\n",
        "# oluşturalacak liste --> ([ word_i-2, word_i-1 ], target word)\n",
        "\n",
        "trigrams = [([test_sentence[i], test_sentence[i + 1]], test_sentence[i + 2])\n",
        "            for i in range(len(test_sentence) - 2)]\n",
        "\n",
        "print(trigrams[:3])\n",
        "\n",
        "vocab = set(test_sentence)\n",
        "word_to_ix = {word: i for i, word in enumerate(vocab)}\n",
        "ix_to_word = {i: word for i, word in enumerate(vocab)}\n",
        "\n",
        "class NGramLanguageModeler(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size, embedding_dim, context_size):\n",
        "        super(NGramLanguageModeler, self).__init__()\n",
        "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.linear1 = nn.Linear(context_size * embedding_dim, 256)\n",
        "        self.linear2 = nn.Linear(256, vocab_size)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "\n",
        "        step_logger.add_info(inputs,\"Inputs\")\n",
        "        embeds = self.embeddings(inputs).view((1, -1))\n",
        "\n",
        "        step_logger.add_info(embeds,\"Embeds\")\n",
        "        out = F.relu(self.linear1(embeds))\n",
        "\n",
        "        step_logger.add_info(out,\"Out\")\n",
        "        out = self.linear2(out)\n",
        "\n",
        "        step_logger.add_info(out,\"Out-2\")\n",
        "        log_probs = F.log_softmax(out, dim=1)\n",
        "\n",
        "        step_logger.add_info(log_probs,\"Out-log_probs\")\n",
        "        return log_probs\n",
        "\n",
        "\n",
        "losses = []\n",
        "loss_function = nn.NLLLoss()\n",
        "step_logger =  StepLogger(2)\n",
        "model = NGramLanguageModeler(len(vocab), EMBEDDING_DIM, CONTEXT_SIZE)\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
        "\n",
        "for epoch in range(10):\n",
        "    total_loss = 0\n",
        "    for context, target in trigrams:\n",
        "\n",
        "#context = when forty\n",
        "#target  = winters\n",
        "\n",
        "        # Step 1. Modele vericek giriş hazırlanıyor..\n",
        "        \n",
        "        step_logger.add_info(context,\"Giriş Kelimeleri\")\n",
        "        step_logger.add_info(target,\"Hedef Kelimeler\")\n",
        "        #print(\"Context\" , context)\n",
        "        #print(\"Target : \" , target)\n",
        "\n",
        "        context_idxs = torch.tensor([word_to_ix[w] for w in context], dtype=torch.long)\n",
        "\n",
        "        #print(\"context_idxs\" , context_idxs)\n",
        "\n",
        "#context_idxs = [0 , 40]\n",
        "\n",
        "        # Step 2. Her döngüde gradı sıfırlamak için unutma !!\n",
        "\n",
        "        model.zero_grad()\n",
        "        step_logger.add_info(context_idxs,\"Input ID\")\n",
        "        # Step 3. Modeli Çalıştırıyoruz\n",
        "        \n",
        "        log_probs = model(context_idxs)\n",
        "        \n",
        "# verilen iki kelimeden sonra gelebilcek kelimelerin ilişkisini tahmin etmeye çalıştık. döndürdüğü matriste her bir kelime için puan hesapladı. puanı en yüksek kelime ikiliden sonra gelebilcek kelimedir.\n",
        "\n",
        "        #print(\"log_probs\" , log_probs)\n",
        "\n",
        "        # Step 4. Loss Fonksiyonu Çağrıldı Ve Loss ne kadar olduğunu gördük.\n",
        "        \n",
        "        loss = loss_function(log_probs, torch.tensor([word_to_ix[target]], dtype=torch.long))\n",
        "        step_logger.add_info(log_probs,\"Çıkış Boyutu\")\n",
        "# hata miktarımızı gördük\n",
        "        #print(\"loss\" , loss)\n",
        "\n",
        "        # Step 5. Geri Yayılım Ve Güncelleme\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Lossu int yaptık.\n",
        "        total_loss += loss.item()\n",
        "    losses.append(total_loss)\n",
        "print(losses)  # The loss decreased every iteration over the training data!\n",
        "\n",
        "with torch.no_grad():\n",
        "    context = ['When' ,'forty']# hedef 'abstract'\n",
        "    context_idxs = torch.tensor([word_to_ix[w] for w in context], dtype=torch.long)\n",
        "    predict = model(context_idxs)\n",
        "    \n",
        "    print(ix_to_word[predict.argmax(dim=1).item()])\n",
        "    print(\"prdict\" , predict)\n",
        "\n",
        "    print(torch.sort(predict))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[(['When', 'forty'], 'winters'), (['forty', 'winters'], 'shall'), (['winters', 'shall'], 'besiege')]\n",
            "[579.613226890564, 574.3146271705627, 569.0824315547943, 563.9145669937134, 558.8098919391632, 553.7644205093384, 548.7760677337646, 543.842545747757, 538.9630312919617, 534.1353814601898]\n",
            "Summer\n",
            "prdict tensor([[-4.5998, -4.9291, -5.0999, -4.6093, -4.1772, -4.8009, -4.4914, -4.6314,\n",
            "         -4.7853, -4.7526, -4.5573, -4.6274, -4.3923, -4.8827, -4.7588, -4.6507,\n",
            "         -5.0153, -4.4443, -4.2072, -4.7182, -4.5671, -4.2100, -4.4596, -4.8192,\n",
            "         -4.2911, -4.8901, -4.5645, -5.1531, -4.6152, -4.4792, -4.7092, -4.7502,\n",
            "         -5.0860, -4.3482, -4.3498, -4.5491, -4.3841, -4.9810, -4.8751, -4.9382,\n",
            "         -4.7898, -4.6182, -4.4279, -4.6396, -4.7201, -4.4722, -4.5700, -4.8065,\n",
            "         -4.8564, -4.9501, -4.6754, -4.9228, -4.4519, -4.6795, -4.8671, -4.7060,\n",
            "         -4.8989, -4.4131, -4.5746, -4.6581, -4.4561, -4.9229, -4.4557, -5.1387,\n",
            "         -4.6617, -4.6315, -4.6226, -5.2032, -4.7968, -4.0556, -4.1876, -3.0129,\n",
            "         -4.5798, -4.6648, -4.6505, -4.4420, -4.5691, -4.9257, -4.7435, -4.6443,\n",
            "         -4.8737, -4.6059, -4.5531, -4.8517, -4.3285, -4.0628, -4.6722, -4.5901,\n",
            "         -4.6628, -4.5901, -5.2131, -4.4298, -4.4872, -4.7874, -4.6211, -5.0083,\n",
            "         -4.2959, -4.6873]])\n",
            "torch.return_types.sort(\n",
            "values=tensor([[-5.2131, -5.2032, -5.1531, -5.1387, -5.0999, -5.0860, -5.0153, -5.0083,\n",
            "         -4.9810, -4.9501, -4.9382, -4.9291, -4.9257, -4.9229, -4.9228, -4.8989,\n",
            "         -4.8901, -4.8827, -4.8751, -4.8737, -4.8671, -4.8564, -4.8517, -4.8192,\n",
            "         -4.8065, -4.8009, -4.7968, -4.7898, -4.7874, -4.7853, -4.7588, -4.7526,\n",
            "         -4.7502, -4.7435, -4.7201, -4.7182, -4.7092, -4.7060, -4.6873, -4.6795,\n",
            "         -4.6754, -4.6722, -4.6648, -4.6628, -4.6617, -4.6581, -4.6507, -4.6505,\n",
            "         -4.6443, -4.6396, -4.6315, -4.6314, -4.6274, -4.6226, -4.6211, -4.6182,\n",
            "         -4.6152, -4.6093, -4.6059, -4.5998, -4.5901, -4.5901, -4.5798, -4.5746,\n",
            "         -4.5700, -4.5691, -4.5671, -4.5645, -4.5573, -4.5531, -4.5491, -4.4914,\n",
            "         -4.4872, -4.4792, -4.4722, -4.4596, -4.4561, -4.4557, -4.4519, -4.4443,\n",
            "         -4.4420, -4.4298, -4.4279, -4.4131, -4.3923, -4.3841, -4.3498, -4.3482,\n",
            "         -4.3285, -4.2959, -4.2911, -4.2100, -4.2072, -4.1876, -4.1772, -4.0628,\n",
            "         -4.0556, -3.0129]]),\n",
            "indices=tensor([[90, 67, 27, 63,  2, 32, 16, 95, 37, 49, 39,  1, 77, 61, 51, 56, 25, 13,\n",
            "         38, 80, 54, 48, 83, 23, 47,  5, 68, 40, 93,  8, 14,  9, 31, 78, 44, 19,\n",
            "         30, 55, 97, 53, 50, 86, 73, 88, 64, 59, 15, 74, 79, 43, 65,  7, 11, 66,\n",
            "         94, 41, 28,  3, 81,  0, 87, 89, 72, 58, 46, 76, 20, 26, 10, 82, 35,  6,\n",
            "         92, 29, 45, 22, 60, 62, 52, 17, 75, 91, 42, 57, 12, 36, 34, 33, 84, 96,\n",
            "         24, 21, 18, 70,  4, 85, 69, 71]]))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-towi7fSF5U9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c699e601-2984-4c9b-d703-1e490a918404"
      },
      "source": [
        "word_to_ix[\"Summer\"]"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "71"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A819LDvPSonO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "5628cf03-0ffb-4c55-8d40-8bd79f9373a0"
      },
      "source": [
        "word_to_ix"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{\"'This\": 48,\n",
              " 'And': 36,\n",
              " 'How': 17,\n",
              " 'If': 16,\n",
              " 'Proving': 31,\n",
              " 'Shall': 37,\n",
              " 'Summer': 71,\n",
              " 'Then': 26,\n",
              " 'This': 74,\n",
              " 'Thy': 97,\n",
              " 'To': 78,\n",
              " 'Were': 76,\n",
              " 'When': 21,\n",
              " 'Where': 77,\n",
              " 'Will': 60,\n",
              " 'a': 19,\n",
              " 'all': 12,\n",
              " 'all-eating': 84,\n",
              " 'an': 38,\n",
              " 'and': 24,\n",
              " 'answer': 7,\n",
              " 'art': 25,\n",
              " 'asked,': 95,\n",
              " 'be': 46,\n",
              " 'beauty': 40,\n",
              " \"beauty's\": 79,\n",
              " 'being': 59,\n",
              " 'besiege': 73,\n",
              " 'blood': 83,\n",
              " 'brow,': 30,\n",
              " 'by': 81,\n",
              " 'child': 72,\n",
              " 'cold.': 4,\n",
              " 'couldst': 1,\n",
              " 'count,': 94,\n",
              " 'days;': 51,\n",
              " 'deep': 10,\n",
              " \"deserv'd\": 3,\n",
              " 'dig': 91,\n",
              " \"excuse,'\": 75,\n",
              " 'eyes,': 18,\n",
              " 'fair': 23,\n",
              " \"feel'st\": 45,\n",
              " 'field,': 11,\n",
              " 'forty': 69,\n",
              " 'gazed': 82,\n",
              " 'held:': 2,\n",
              " 'his': 86,\n",
              " 'in': 0,\n",
              " 'it': 57,\n",
              " 'lies,': 49,\n",
              " 'livery': 70,\n",
              " 'lusty': 63,\n",
              " 'made': 5,\n",
              " 'make': 39,\n",
              " 'mine': 53,\n",
              " 'more': 13,\n",
              " 'much': 87,\n",
              " 'my': 88,\n",
              " 'new': 50,\n",
              " 'now,': 90,\n",
              " 'of': 42,\n",
              " 'old': 92,\n",
              " 'old,': 20,\n",
              " 'on': 89,\n",
              " 'own': 28,\n",
              " 'praise': 29,\n",
              " 'praise.': 68,\n",
              " 'proud': 6,\n",
              " 'say,': 61,\n",
              " 'see': 34,\n",
              " 'shall': 66,\n",
              " 'shame,': 56,\n",
              " 'small': 44,\n",
              " 'so': 96,\n",
              " 'succession': 43,\n",
              " 'sum': 52,\n",
              " 'sunken': 8,\n",
              " 'the': 64,\n",
              " 'thine': 32,\n",
              " 'thine!': 14,\n",
              " 'thou': 62,\n",
              " 'thriftless': 9,\n",
              " 'thy': 85,\n",
              " 'to': 93,\n",
              " \"totter'd\": 15,\n",
              " 'treasure': 41,\n",
              " 'trenches': 22,\n",
              " 'use,': 65,\n",
              " 'warm': 58,\n",
              " 'weed': 55,\n",
              " 'were': 27,\n",
              " 'when': 54,\n",
              " 'where': 47,\n",
              " 'winters': 33,\n",
              " 'within': 80,\n",
              " 'worth': 67,\n",
              " \"youth's\": 35}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I2DJSSb9uxVx",
        "colab_type": "text"
      },
      "source": [
        "## **C-Bow**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Cn11Ba8wble",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "4d97009a-a941-4c12-95c1-717cabc59c85"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "torch.manual_seed(1)\n",
        "\n",
        "CONTEXT_SIZE = 2  # 2 sağ 2 sol bakıcaz\n",
        "raw_text = \"\"\"We are about to study the idea of a computational process.\n",
        "Computational processes are abstract beings that inhabit computers.\n",
        "As they evolve, processes manipulate other abstract things called data.\n",
        "The evolution of a process is directed by a pattern of rules\n",
        "called a program. People create programs to direct processes. In effect,\n",
        "we conjure the spirits of the computer with our spells.\"\"\".split()\n",
        "\n",
        "\n",
        "vocab = set(raw_text)\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "word_to_ix = {word: i for i, word in enumerate(vocab)}\n",
        "ix_to_word = {i: word for i, word in enumerate(vocab)}\n",
        "\n",
        "data = []\n",
        "for i in range(2, len(raw_text) - 2):\n",
        "    context = [raw_text[i - 2], raw_text[i - 1],\n",
        "               raw_text[i + 1], raw_text[i + 2]]\n",
        "    target = raw_text[i]\n",
        "    data.append((context, target))\n",
        "print(data[:5])\n",
        "\n",
        "class CBOW(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim):\n",
        "        super(CBOW, self).__init__()\n",
        "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.linear1 = nn.Linear(embedding_dim, 128)\n",
        "        self.linear2 = nn.Linear(128, vocab_size)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "\n",
        "        step_logger.add_info(inputs,\"Inputs\")\n",
        "        embeds = self.embeddings(inputs)\n",
        "        \n",
        "        step_logger.add_info(embeds,\"Embeds\")\n",
        "        embeds = torch.sum(embeds, dim=0).view(1,-1)\n",
        "        step_logger.add_info(embeds,\"Embeds\")\n",
        "\n",
        "        \n",
        "        out = F.relu(self.linear1(embeds))\n",
        "        step_logger.add_info(out,\"Out\")\n",
        "\n",
        "        \n",
        "        out = self.linear2(out)\n",
        "        step_logger.add_info(out,\"Out - 2\")\n",
        "        \n",
        "        log_probs = F.log_softmax(out, dim=1)\n",
        "        step_logger.add_info(log_probs,\"Out - log_probs\")\n",
        "        return log_probs\n",
        "\n",
        "\n",
        "def make_context_vector(context, word_to_ix):\n",
        "    idxs = [word_to_ix[w] for w in context]\n",
        "    return torch.tensor(idxs, dtype=torch.long)\n",
        "\n",
        "losses = []\n",
        "loss_function = nn.NLLLoss()\n",
        "model = CBOW(vocab_size, embedding_dim=20)\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
        "\n",
        "for epoch in range(200):\n",
        "    total_loss = 0\n",
        "    for context, target in data:\n",
        "\n",
        "        # Step 1. Modele vericek giriş hazırlanıyor..\n",
        "        step_logger.add_info(context,\"Giriş Kelimeleri\")\n",
        "        step_logger.add_info(target,\"Hedef Kelimeler\")\n",
        "\n",
        "        context_idxs = make_context_vector(context, word_to_ix)\n",
        "        step_logger.add_info(context_idxs,\"Giriş ID'leri\")\n",
        "        \n",
        "\n",
        "        \n",
        "        # Step 2. Her döngüde gradı sıfırlamak için unutma !!\n",
        "\n",
        "        model.zero_grad()\n",
        "        \n",
        "        #  Step 3. Modeli Çalıştırıyoruz\n",
        "\n",
        "        log_probs = model(context_idxs)\n",
        "        step_logger.add_info(log_probs,\"Çıkış\")\n",
        "        # Loss hesaplanıyor\n",
        "        loss = loss_function(log_probs, torch.tensor([word_to_ix[target]], dtype=torch.long))\n",
        "\n",
        "        # Step 5. Geri Yayılım Ve Güncelleme\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        \n",
        "        total_loss += loss.item()\n",
        "\n",
        "    losses.append(total_loss)\n",
        "\n",
        "print(losses)  \n",
        "\n",
        "# Test\n",
        "with torch.no_grad():\n",
        "    context = ['manipulate', 'other', 'things', 'called']# hedef 'abstract'\n",
        "    context_vector = make_context_vector(context, word_to_ix)\n",
        "    predict = model(context_vector)\n",
        "    print(ix_to_word[predict.argmax(dim=1).item()])\n",
        "    "
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[(['We', 'are', 'to', 'study'], 'about'), (['are', 'about', 'study', 'the'], 'to'), (['about', 'to', 'the', 'idea'], 'study'), (['to', 'study', 'idea', 'of'], 'the'), (['study', 'the', 'of', 'a'], 'idea')]\n",
            "[231.63839769363403, 226.3545880317688, 221.23607397079468, 216.2794132232666, 211.46721816062927, 206.7799129486084, 202.222083568573, 197.78033781051636, 193.45168137550354, 189.2250578403473, 185.09620904922485, 181.06700456142426, 177.12904596328735, 173.28111684322357, 169.52237379550934, 165.84369313716888, 162.24597692489624, 158.7208503484726, 155.26600241661072, 151.87890553474426, 148.55104756355286, 145.28077971935272, 142.06333017349243, 138.9022661447525, 135.7928510904312, 132.7371107339859, 129.72949528694153, 126.76932978630066, 123.85904824733734, 121.0003473162651, 118.18883925676346, 115.42624920606613, 112.71280032396317, 110.04614990949631, 107.43204808235168, 104.86308205127716, 102.34725445508957, 99.88433456420898, 97.46948248147964, 95.09938055276871, 92.78188395500183, 90.51117265224457, 88.29054093360901, 86.12006241083145, 83.99918872117996, 81.92680990695953, 79.90080827474594, 77.92711538076401, 75.99771144986153, 74.11827158927917, 72.28707945346832, 70.4999218583107, 68.7571362555027, 67.05691093206406, 65.40026378631592, 63.78549003601074, 62.21318221092224, 60.68198621273041, 59.19138461351395, 57.741196781396866, 56.328405916690826, 54.95674532651901, 53.62116113305092, 52.32359114289284, 51.06011363863945, 49.833321422338486, 48.6381796002388, 47.47874474525452, 46.348723351955414, 45.25262039899826, 44.185602486133575, 43.14902827143669, 42.14204978942871, 41.16224080324173, 40.208929032087326, 39.28189769387245, 38.38049851357937, 37.504932940006256, 36.65357427299023, 35.826841309666634, 35.0232579857111, 34.241673812270164, 33.48417979478836, 32.745967760682106, 32.02994084358215, 31.33324997127056, 30.65687596797943, 29.999526172876358, 29.36013351380825, 28.738449573516846, 28.134337857365608, 27.547581523656845, 26.976521119475365, 26.422171041369438, 25.882873371243477, 25.359507098793983, 24.849301621317863, 24.354303151369095, 23.87183752655983, 23.40336997807026, 22.947812408208847, 22.504409819841385, 22.073504701256752, 21.655068039894104, 21.246336087584496, 20.850505262613297, 20.46435311436653, 20.088842913508415, 19.72336518764496, 19.367580257356167, 19.021275624632835, 18.684175238013268, 18.355595350265503, 18.035767696797848, 17.72408489882946, 17.42048069089651, 17.12500348687172, 16.83671160042286, 16.55566481500864, 16.281990729272366, 16.014599584043026, 15.754609040915966, 15.50085625052452, 15.25322987884283, 15.01196999847889, 14.776411645114422, 14.546280682086945, 14.322341233491898, 14.103332243859768, 13.88953473418951, 13.68095713853836, 13.477457992732525, 13.278445765376091, 13.084409795701504, 12.894972950220108, 12.709450788795948, 12.528756640851498, 12.352093681693077, 12.17908350378275, 12.01028212159872, 11.845371998846531, 11.683911360800266, 11.526420339941978, 11.371840193867683, 11.221134305000305, 11.073357850313187, 10.928646869957447, 10.787570476531982, 10.648958876729012, 10.513402454555035, 10.380787551403046, 10.250672847032547, 10.123398959636688, 9.998892202973366, 9.876553907990456, 9.75703800842166, 9.63961710035801, 9.524527061730623, 9.412054128944874, 9.301418535411358, 9.193092003464699, 9.086876273155212, 8.982703570276499, 8.880507994443178, 8.780188392847776, 8.682067804038525, 8.58535823598504, 8.491023927927017, 8.398029010742903, 8.306687116622925, 8.217387359589338, 8.129362158477306, 8.04293854162097, 7.95851169899106, 7.875208776444197, 7.793564483523369, 7.7132234163582325, 7.634424094110727, 7.556921813637018, 7.480832774192095, 7.405876856297255, 7.332488942891359, 7.260104205459356, 7.189104791730642, 7.119209319353104, 7.050479996949434, 6.982968300580978, 6.916489914059639, 6.851138573139906, 6.7868664637207985, 6.7235804460942745, 6.661333169788122, 6.600143667310476, 6.539717011153698, 6.480469200760126, 6.422067683190107, 6.364470805972815, 6.3080202005803585, 6.25219177827239, 6.197322979569435]\n",
            "abstract\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}