{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train = make_classification(n_samples=1000,n_features=2, n_redundant=0, n_informative=1,\n",
    "                             n_clusters_per_class=1)\n",
    "x_train = torch.FloatTensor(x_train)\n",
    "y_train = torch.FloatTensor(y_train)\n",
    "\n",
    "x_test, y_test = make_classification(n_samples=100,n_features=2, n_redundant=0, n_informative=1,\n",
    "                             n_clusters_per_class=1)\n",
    "x_test = torch.FloatTensor(x_test)\n",
    "y_test = torch.FloatTensor(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1.,\n",
       "        1., 0., 1., 1., 0., 1., 0., 0., 0., 1., 1., 0., 0., 1., 0., 0., 0., 1.,\n",
       "        0., 1., 0., 1., 1., 1., 0., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 0.,\n",
       "        1., 1., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0., 1.,\n",
       "        1., 1., 0., 1., 1., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 1.,\n",
       "        1., 1., 0., 0., 0., 0., 0., 1., 0., 0.])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (fc1): Linear(in_features=2, out_features=16, bias=True)\n",
      "  (fc2): Linear(in_features=16, out_features=1, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      "  (relu): ReLU()\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(2, 16)\n",
    "        self.fc2 = nn.Linear(16, 1)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()  \n",
    "                \n",
    "    def forward(self, x):\n",
    "        \n",
    "        \n",
    "        x = self.fc1(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.sigmoid(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "model = Net()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: train loss: 0.7353194355964661\n",
      "Epoch 0: train loss: 0.7136341333389282\n",
      "Epoch 0: train loss: 0.6761712431907654\n",
      "Epoch 0: train loss: 0.6469497084617615\n",
      "Epoch 0: train loss: 0.5942229628562927\n",
      "Epoch 0: train loss: 0.6223369836807251\n",
      "Epoch 0: train loss: 0.6378325819969177\n",
      "Epoch 0: train loss: 0.611420214176178\n",
      "Epoch 0: train loss: 0.6233415007591248\n",
      "Epoch 0: train loss: 0.6116925477981567\n",
      "Epoch 0: train loss: 0.5539445877075195\n",
      "Epoch 0: train loss: 0.5505024194717407\n",
      "Epoch 0: train loss: 0.5416257381439209\n",
      "Epoch 0: train loss: 0.5822339057922363\n",
      "Epoch 0: train loss: 0.6111814975738525\n",
      "Epoch 0: train loss: 0.47178003191947937\n",
      "Epoch 0: train loss: 0.5891743302345276\n",
      "Epoch 0: train loss: 0.4790463149547577\n",
      "Epoch 0: train loss: 0.4672233760356903\n",
      "Epoch 0: train loss: 0.5434008836746216\n",
      "Epoch 0: train loss: 0.4944612979888916\n",
      "Epoch 0: train loss: 0.4375859498977661\n",
      "Epoch 0: train loss: 0.5616117715835571\n",
      "Epoch 0: train loss: 0.5986526608467102\n",
      "Epoch 0: train loss: 0.4895398020744324\n",
      "Epoch 0: train loss: 0.43042007088661194\n",
      "Epoch 0: train loss: 0.34449222683906555\n",
      "Epoch 0: train loss: 0.41240352392196655\n",
      "Epoch 0: train loss: 0.541343092918396\n",
      "Epoch 0: train loss: 0.49387872219085693\n",
      "Epoch 0: train loss: 0.364179402589798\n",
      "Epoch 0: train loss: 0.5232110023498535\n",
      "Epoch 0: train loss: 0.5984993577003479\n",
      "Epoch 0: train loss: 0.29570645093917847\n",
      "Epoch 0: train loss: 0.3863798975944519\n",
      "Epoch 0: train loss: 0.4409160315990448\n",
      "Epoch 0: train loss: 0.42991751432418823\n",
      "Epoch 0: train loss: 0.3856132924556732\n",
      "Epoch 0: train loss: 0.45957067608833313\n",
      "Epoch 0: train loss: 0.2903144657611847\n",
      "Epoch 0: train loss: 0.2798726558685303\n",
      "Epoch 0: train loss: 0.41435980796813965\n",
      "Epoch 0: train loss: 0.3143680691719055\n",
      "Epoch 0: train loss: 0.3858937919139862\n",
      "Epoch 0: train loss: 0.4283580780029297\n",
      "Epoch 0: train loss: 0.41601094603538513\n",
      "Epoch 0: train loss: 0.4641711413860321\n",
      "Epoch 0: train loss: 0.3299359679222107\n",
      "Epoch 0: train loss: 0.5019225478172302\n",
      "Epoch 0: train loss: 0.5111057162284851\n",
      "Epoch 0: train loss: 0.4909650385379791\n",
      "Epoch 0: train loss: 0.413914293050766\n",
      "Epoch 0: train loss: 0.3256498873233795\n",
      "Epoch 0: train loss: 0.5060912370681763\n",
      "Epoch 0: train loss: 0.6599612236022949\n",
      "Epoch 0: train loss: 0.4117085933685303\n",
      "Epoch 0: train loss: 0.3150274157524109\n",
      "Epoch 0: train loss: 0.41286805272102356\n",
      "Epoch 0: train loss: 0.5236634016036987\n",
      "Epoch 0: train loss: 0.46174585819244385\n",
      "Epoch 0: train loss: 0.3491538166999817\n",
      "Epoch 0: train loss: 0.4896834194660187\n",
      "Validation loss decreased (0.000000 ===> 0.489683). Saving the model...\n",
      "Epoch 1: train loss: 0.4046288728713989\n",
      "Epoch 1: train loss: 0.41741740703582764\n",
      "Epoch 1: train loss: 0.43939676880836487\n",
      "Epoch 1: train loss: 0.28557950258255005\n",
      "Epoch 1: train loss: 0.27337634563446045\n",
      "Epoch 1: train loss: 0.3672653138637543\n",
      "Epoch 1: train loss: 0.5125449299812317\n",
      "Epoch 1: train loss: 0.36151695251464844\n",
      "Epoch 1: train loss: 0.47792482376098633\n",
      "Epoch 1: train loss: 0.430915504693985\n",
      "Epoch 1: train loss: 0.32781994342803955\n",
      "Epoch 1: train loss: 0.375632643699646\n",
      "Epoch 1: train loss: 0.3658941686153412\n",
      "Epoch 1: train loss: 0.42035117745399475\n",
      "Epoch 1: train loss: 0.4937206208705902\n",
      "Epoch 1: train loss: 0.274432897567749\n",
      "Epoch 1: train loss: 0.4398415684700012\n",
      "Epoch 1: train loss: 0.32359442114830017\n",
      "Epoch 1: train loss: 0.37960872054100037\n",
      "Epoch 1: train loss: 0.43668848276138306\n",
      "Epoch 1: train loss: 0.40423163771629333\n",
      "Epoch 1: train loss: 0.33874213695526123\n",
      "Epoch 1: train loss: 0.48364394903182983\n",
      "Epoch 1: train loss: 0.500067949295044\n",
      "Epoch 1: train loss: 0.39980262517929077\n",
      "Epoch 1: train loss: 0.39316222071647644\n",
      "Epoch 1: train loss: 0.24858252704143524\n",
      "Epoch 1: train loss: 0.3265038728713989\n",
      "Epoch 1: train loss: 0.48705586791038513\n",
      "Epoch 1: train loss: 0.41725754737854004\n",
      "Epoch 1: train loss: 0.239846333861351\n",
      "Epoch 1: train loss: 0.49079465866088867\n",
      "Epoch 1: train loss: 1.0236132144927979\n",
      "Epoch 1: train loss: 0.19753751158714294\n",
      "Epoch 1: train loss: 0.29809775948524475\n",
      "Epoch 1: train loss: 0.39833706617355347\n",
      "Epoch 1: train loss: 0.37399423122406006\n",
      "Epoch 1: train loss: 0.35183820128440857\n",
      "Epoch 1: train loss: 0.4397294521331787\n",
      "Epoch 1: train loss: 0.24327842891216278\n",
      "Epoch 1: train loss: 0.2555111348628998\n",
      "Epoch 1: train loss: 0.36002644896507263\n",
      "Epoch 1: train loss: 0.27914485335350037\n",
      "Epoch 1: train loss: 0.3669013977050781\n",
      "Epoch 1: train loss: 0.4179540276527405\n",
      "Epoch 1: train loss: 0.39297324419021606\n",
      "Epoch 1: train loss: 0.44065144658088684\n",
      "Epoch 1: train loss: 0.30742573738098145\n",
      "Epoch 1: train loss: 0.47521349787712097\n",
      "Epoch 1: train loss: 0.4500608742237091\n",
      "Epoch 1: train loss: 0.4802292585372925\n",
      "Epoch 1: train loss: 0.38596850633621216\n",
      "Epoch 1: train loss: 0.3122124969959259\n",
      "Epoch 1: train loss: 0.4843897223472595\n",
      "Epoch 1: train loss: 0.657296895980835\n",
      "Epoch 1: train loss: 0.39289426803588867\n",
      "Epoch 1: train loss: 0.313883900642395\n",
      "Epoch 1: train loss: 0.3978325128555298\n",
      "Epoch 1: train loss: 0.5241647958755493\n",
      "Epoch 1: train loss: 0.4214254319667816\n",
      "Epoch 1: train loss: 0.31717222929000854\n",
      "Epoch 1: train loss: 0.4865643382072449\n",
      "Epoch 2: train loss: 0.39405128359794617\n",
      "Epoch 2: train loss: 0.4110945165157318\n",
      "Epoch 2: train loss: 0.3856976628303528\n",
      "Epoch 2: train loss: 0.26995301246643066\n",
      "Epoch 2: train loss: 0.26695072650909424\n",
      "Epoch 2: train loss: 0.35648012161254883\n",
      "Epoch 2: train loss: 0.5104390382766724\n",
      "Epoch 2: train loss: 0.3396901786327362\n",
      "Epoch 2: train loss: 0.477042555809021\n",
      "Epoch 2: train loss: 0.40620148181915283\n",
      "Epoch 2: train loss: 0.2830738425254822\n",
      "Epoch 2: train loss: 0.3578602075576782\n",
      "Epoch 2: train loss: 0.3409656584262848\n",
      "Epoch 2: train loss: 0.39851245284080505\n",
      "Epoch 2: train loss: 0.47819238901138306\n",
      "Epoch 2: train loss: 0.265466570854187\n",
      "Epoch 2: train loss: 0.4131574034690857\n",
      "Epoch 2: train loss: 0.30829185247421265\n",
      "Epoch 2: train loss: 0.34917691349983215\n",
      "Epoch 2: train loss: 0.4122990369796753\n",
      "Epoch 2: train loss: 0.3770062029361725\n",
      "Epoch 2: train loss: 0.3168826103210449\n",
      "Epoch 2: train loss: 0.47853556275367737\n",
      "Epoch 2: train loss: 0.5085049271583557\n",
      "Epoch 2: train loss: 0.40189024806022644\n",
      "Epoch 2: train loss: 0.39000895619392395\n",
      "Epoch 2: train loss: 0.23627378046512604\n",
      "Epoch 2: train loss: 0.3218526840209961\n",
      "Epoch 2: train loss: 0.47861284017562866\n",
      "Epoch 2: train loss: 0.4006493389606476\n",
      "Epoch 2: train loss: 0.22702747583389282\n",
      "Epoch 2: train loss: 0.4784608781337738\n",
      "Epoch 2: train loss: 0.9998262524604797\n",
      "Epoch 2: train loss: 0.18416723608970642\n",
      "Epoch 2: train loss: 0.2938389480113983\n",
      "Epoch 2: train loss: 0.3996618688106537\n",
      "Epoch 2: train loss: 0.3816492259502411\n",
      "Epoch 2: train loss: 0.34697210788726807\n",
      "Epoch 2: train loss: 0.4347383379936218\n",
      "Epoch 2: train loss: 0.2384929656982422\n",
      "Epoch 2: train loss: 0.24456575512886047\n",
      "Epoch 2: train loss: 0.3531116247177124\n",
      "Epoch 2: train loss: 0.2751724123954773\n",
      "Epoch 2: train loss: 0.35057395696640015\n",
      "Epoch 2: train loss: 0.42114686965942383\n",
      "Epoch 2: train loss: 0.39124760031700134\n",
      "Epoch 2: train loss: 0.44214874505996704\n",
      "Epoch 2: train loss: 0.3083887994289398\n",
      "Epoch 2: train loss: 0.4531448781490326\n",
      "Epoch 2: train loss: 0.447337806224823\n",
      "Epoch 2: train loss: 0.4735746681690216\n",
      "Epoch 2: train loss: 0.36803269386291504\n",
      "Epoch 2: train loss: 0.30699849128723145\n",
      "Epoch 2: train loss: 0.48274776339530945\n",
      "Epoch 2: train loss: 0.630109429359436\n",
      "Epoch 2: train loss: 0.3923603296279907\n",
      "Epoch 2: train loss: 0.3125155568122864\n",
      "Epoch 2: train loss: 0.40859028697013855\n",
      "Epoch 2: train loss: 0.526732861995697\n",
      "Epoch 2: train loss: 0.4144511818885803\n",
      "Epoch 2: train loss: 0.309660941362381\n",
      "Epoch 2: train loss: 0.4891810417175293\n",
      "Epoch 3: train loss: 0.3974224030971527\n",
      "Epoch 3: train loss: 0.3950237035751343\n",
      "Epoch 3: train loss: 0.3662135601043701\n",
      "Epoch 3: train loss: 0.26357758045196533\n",
      "Epoch 3: train loss: 0.26080983877182007\n",
      "Epoch 3: train loss: 0.3597401976585388\n",
      "Epoch 3: train loss: 0.5034000873565674\n",
      "Epoch 3: train loss: 0.3283936679363251\n",
      "Epoch 3: train loss: 0.47964996099472046\n",
      "Epoch 3: train loss: 0.3937555253505707\n",
      "Epoch 3: train loss: 0.29361218214035034\n",
      "Epoch 3: train loss: 0.3561099171638489\n",
      "Epoch 3: train loss: 0.34490451216697693\n",
      "Epoch 3: train loss: 0.39476487040519714\n",
      "Epoch 3: train loss: 0.48498478531837463\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: train loss: 0.2624231278896332\n",
      "Epoch 3: train loss: 0.403995543718338\n",
      "Epoch 3: train loss: 0.3145504891872406\n",
      "Epoch 3: train loss: 0.3479103446006775\n",
      "Epoch 3: train loss: 0.40217334032058716\n",
      "Epoch 3: train loss: 0.3681045472621918\n",
      "Epoch 3: train loss: 0.3288324177265167\n",
      "Epoch 3: train loss: 0.4787605106830597\n",
      "Epoch 3: train loss: 0.49271005392074585\n",
      "Epoch 3: train loss: 0.3930360972881317\n",
      "Epoch 3: train loss: 0.39265501499176025\n",
      "Epoch 3: train loss: 0.23039443790912628\n",
      "Epoch 3: train loss: 0.3185511827468872\n",
      "Epoch 3: train loss: 0.4823909103870392\n",
      "Epoch 3: train loss: 0.4010315239429474\n",
      "Epoch 3: train loss: 0.2291402369737625\n",
      "Epoch 3: train loss: 0.4794270098209381\n",
      "Epoch 3: train loss: 1.0657322406768799\n",
      "Epoch 3: train loss: 0.17978140711784363\n",
      "Epoch 3: train loss: 0.280071496963501\n",
      "Epoch 3: train loss: 0.39098337292671204\n",
      "Epoch 3: train loss: 0.37567004561424255\n",
      "Epoch 3: train loss: 0.34751608967781067\n",
      "Epoch 3: train loss: 0.4269651174545288\n",
      "Epoch 3: train loss: 0.24096854031085968\n",
      "Epoch 3: train loss: 0.2557121515274048\n",
      "Epoch 3: train loss: 0.35048481822013855\n",
      "Epoch 3: train loss: 0.26788267493247986\n",
      "Epoch 3: train loss: 0.3486122190952301\n",
      "Epoch 3: train loss: 0.4074901342391968\n",
      "Epoch 3: train loss: 0.39127790927886963\n",
      "Epoch 3: train loss: 0.4549318552017212\n",
      "Epoch 3: train loss: 0.30785614252090454\n",
      "Epoch 3: train loss: 0.4595147371292114\n",
      "Epoch 3: train loss: 0.44154807925224304\n",
      "Epoch 3: train loss: 0.4785214066505432\n",
      "Epoch 3: train loss: 0.3676913380622864\n",
      "Epoch 3: train loss: 0.30394500494003296\n",
      "Epoch 3: train loss: 0.4790739417076111\n",
      "Epoch 3: train loss: 0.6182034015655518\n",
      "Epoch 3: train loss: 0.39234015345573425\n",
      "Epoch 3: train loss: 0.322200208902359\n",
      "Epoch 3: train loss: 0.39497292041778564\n",
      "Epoch 3: train loss: 0.5200502276420593\n",
      "Epoch 3: train loss: 0.4127047061920166\n",
      "Epoch 3: train loss: 0.31181612610816956\n",
      "Epoch 3: train loss: 0.494998574256897\n",
      "Validation loss decreased (0.489683 ===> 0.494999). Saving the model...\n",
      "Epoch 4: train loss: 0.3912641406059265\n",
      "Epoch 4: train loss: 0.3923589289188385\n",
      "Epoch 4: train loss: 0.38526272773742676\n",
      "Epoch 4: train loss: 0.2681191861629486\n",
      "Epoch 4: train loss: 0.2616102695465088\n",
      "Epoch 4: train loss: 0.3530723750591278\n",
      "Epoch 4: train loss: 0.49932071566581726\n",
      "Epoch 4: train loss: 0.32364192605018616\n",
      "Epoch 4: train loss: 0.47772425413131714\n",
      "Epoch 4: train loss: 0.40231552720069885\n",
      "Epoch 4: train loss: 0.2744380831718445\n",
      "Epoch 4: train loss: 0.35663679242134094\n",
      "Epoch 4: train loss: 0.3398778736591339\n",
      "Epoch 4: train loss: 0.4014779031276703\n",
      "Epoch 4: train loss: 0.4797325134277344\n",
      "Epoch 4: train loss: 0.26281240582466125\n",
      "Epoch 4: train loss: 0.40707018971443176\n",
      "Epoch 4: train loss: 0.2937053442001343\n",
      "Epoch 4: train loss: 0.356131374835968\n",
      "Epoch 4: train loss: 0.40725696086883545\n",
      "Epoch 4: train loss: 0.37037193775177\n",
      "Epoch 4: train loss: 0.3196742832660675\n",
      "Epoch 4: train loss: 0.4777285158634186\n",
      "Epoch 4: train loss: 0.492099791765213\n",
      "Epoch 4: train loss: 0.40120431780815125\n",
      "Epoch 4: train loss: 0.39067164063453674\n",
      "Epoch 4: train loss: 0.22420144081115723\n",
      "Epoch 4: train loss: 0.31371402740478516\n",
      "Epoch 4: train loss: 0.4773033559322357\n",
      "Epoch 4: train loss: 0.3980889916419983\n",
      "Epoch 4: train loss: 0.22910919785499573\n",
      "Epoch 4: train loss: 0.47686928510665894\n",
      "Epoch 4: train loss: 0.9641067981719971\n",
      "Epoch 4: train loss: 0.18114887177944183\n",
      "Epoch 4: train loss: 0.27917909622192383\n",
      "Epoch 4: train loss: 0.3922784924507141\n",
      "Epoch 4: train loss: 0.358799010515213\n",
      "Epoch 4: train loss: 0.3472450375556946\n",
      "Epoch 4: train loss: 0.4336323142051697\n",
      "Epoch 4: train loss: 0.23113581538200378\n",
      "Epoch 4: train loss: 0.247385635972023\n",
      "Epoch 4: train loss: 0.35461822152137756\n",
      "Epoch 4: train loss: 0.27685868740081787\n",
      "Epoch 4: train loss: 0.3482500910758972\n",
      "Epoch 4: train loss: 0.40776628255844116\n",
      "Epoch 4: train loss: 0.40499380230903625\n",
      "Epoch 4: train loss: 0.4362190365791321\n",
      "Epoch 4: train loss: 0.30541354417800903\n",
      "Epoch 4: train loss: 0.44131022691726685\n",
      "Epoch 4: train loss: 0.44709402322769165\n",
      "Epoch 4: train loss: 0.4510138928890228\n",
      "Epoch 4: train loss: 0.36298805475234985\n",
      "Epoch 4: train loss: 0.3157578408718109\n",
      "Epoch 4: train loss: 0.47841328382492065\n",
      "Epoch 4: train loss: 0.6174585819244385\n",
      "Epoch 4: train loss: 0.3925009071826935\n",
      "Epoch 4: train loss: 0.3103145360946655\n",
      "Epoch 4: train loss: 0.4057699143886566\n",
      "Epoch 4: train loss: 0.5199064612388611\n",
      "Epoch 4: train loss: 0.4226597845554352\n",
      "Epoch 4: train loss: 0.30750781297683716\n",
      "Epoch 4: train loss: 0.48299381136894226\n",
      "Epoch 5: train loss: 0.39228495955467224\n",
      "Epoch 5: train loss: 0.3938687741756439\n",
      "Epoch 5: train loss: 0.3666470944881439\n",
      "Epoch 5: train loss: 0.2758899927139282\n",
      "Epoch 5: train loss: 0.2652527391910553\n",
      "Epoch 5: train loss: 0.35268622636795044\n",
      "Epoch 5: train loss: 0.4971126616001129\n",
      "Epoch 5: train loss: 0.32732465863227844\n",
      "Epoch 5: train loss: 0.4766058027744293\n",
      "Epoch 5: train loss: 0.41590240597724915\n",
      "Epoch 5: train loss: 0.25677889585494995\n",
      "Epoch 5: train loss: 0.3552514612674713\n",
      "Epoch 5: train loss: 0.3370324671268463\n",
      "Epoch 5: train loss: 0.3921637237071991\n",
      "Epoch 5: train loss: 0.4623565971851349\n",
      "Epoch 5: train loss: 0.2632139027118683\n",
      "Epoch 5: train loss: 0.41328391432762146\n",
      "Epoch 5: train loss: 0.29444098472595215\n",
      "Epoch 5: train loss: 0.3481561541557312\n",
      "Epoch 5: train loss: 0.39719533920288086\n",
      "Epoch 5: train loss: 0.36288517713546753\n",
      "Epoch 5: train loss: 0.30910906195640564\n",
      "Epoch 5: train loss: 0.47894781827926636\n",
      "Epoch 5: train loss: 0.48735755681991577\n",
      "Epoch 5: train loss: 0.39158928394317627\n",
      "Epoch 5: train loss: 0.3898984491825104\n",
      "Epoch 5: train loss: 0.22126124799251556\n",
      "Epoch 5: train loss: 0.3162095844745636\n",
      "Epoch 5: train loss: 0.4776953160762787\n",
      "Epoch 5: train loss: 0.39760100841522217\n",
      "Epoch 5: train loss: 0.21887050569057465\n",
      "Epoch 5: train loss: 0.47791874408721924\n",
      "Epoch 5: train loss: 1.1068055629730225\n",
      "Epoch 5: train loss: 0.20671750605106354\n",
      "Epoch 5: train loss: 0.27248990535736084\n",
      "Epoch 5: train loss: 0.3905939757823944\n",
      "Epoch 5: train loss: 0.35747030377388\n",
      "Epoch 5: train loss: 0.35432738065719604\n",
      "Epoch 5: train loss: 0.43427324295043945\n",
      "Epoch 5: train loss: 0.23762930929660797\n",
      "Epoch 5: train loss: 0.2329055517911911\n",
      "Epoch 5: train loss: 0.3491862714290619\n",
      "Epoch 5: train loss: 0.26614153385162354\n",
      "Epoch 5: train loss: 0.3491184115409851\n",
      "Epoch 5: train loss: 0.4179587960243225\n",
      "Epoch 5: train loss: 0.39083629846572876\n",
      "Epoch 5: train loss: 0.44650834798812866\n",
      "Epoch 5: train loss: 0.3118330240249634\n",
      "Epoch 5: train loss: 0.4598732590675354\n",
      "Epoch 5: train loss: 0.44151729345321655\n",
      "Epoch 5: train loss: 0.45568808913230896\n",
      "Epoch 5: train loss: 0.3675456643104553\n",
      "Epoch 5: train loss: 0.3035067319869995\n",
      "Epoch 5: train loss: 0.48025092482566833\n",
      "Epoch 5: train loss: 0.6182161569595337\n",
      "Epoch 5: train loss: 0.39290887117385864\n",
      "Epoch 5: train loss: 0.3072623014450073\n",
      "Epoch 5: train loss: 0.398165225982666\n",
      "Epoch 5: train loss: 0.5202513933181763\n",
      "Epoch 5: train loss: 0.4099302589893341\n",
      "Epoch 5: train loss: 0.3097769320011139\n",
      "Epoch 5: train loss: 0.47732752561569214\n",
      "Epoch 6: train loss: 0.3925060033798218\n",
      "Epoch 6: train loss: 0.3929710388183594\n",
      "Epoch 6: train loss: 0.36876532435417175\n",
      "Epoch 6: train loss: 0.26786720752716064\n",
      "Epoch 6: train loss: 0.26226338744163513\n",
      "Epoch 6: train loss: 0.3496648371219635\n",
      "Epoch 6: train loss: 0.5040084719657898\n",
      "Epoch 6: train loss: 0.3209211528301239\n",
      "Epoch 6: train loss: 0.4765518009662628\n",
      "Epoch 6: train loss: 0.3829207122325897\n",
      "Epoch 6: train loss: 0.2665752172470093\n",
      "Epoch 6: train loss: 0.35535311698913574\n",
      "Epoch 6: train loss: 0.33341532945632935\n",
      "Epoch 6: train loss: 0.4047660827636719\n",
      "Epoch 6: train loss: 0.48583680391311646\n",
      "Epoch 6: train loss: 0.26720207929611206\n",
      "Epoch 6: train loss: 0.39270591735839844\n",
      "Epoch 6: train loss: 0.30575060844421387\n",
      "Epoch 6: train loss: 0.3471035063266754\n",
      "Epoch 6: train loss: 0.39427244663238525\n",
      "Epoch 6: train loss: 0.36678585410118103\n",
      "Epoch 6: train loss: 0.3039573132991791\n",
      "Epoch 6: train loss: 0.47867897152900696\n",
      "Epoch 6: train loss: 0.4827723205089569\n",
      "Epoch 6: train loss: 0.39591479301452637\n",
      "Epoch 6: train loss: 0.3899369239807129\n",
      "Epoch 6: train loss: 0.22053933143615723\n",
      "Epoch 6: train loss: 0.30896252393722534\n",
      "Epoch 6: train loss: 0.47778743505477905\n",
      "Epoch 6: train loss: 0.3970928192138672\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: train loss: 0.22367000579833984\n",
      "Epoch 6: train loss: 0.4781154990196228\n",
      "Epoch 6: train loss: 1.3438705205917358\n",
      "Epoch 6: train loss: 0.1812872737646103\n",
      "Epoch 6: train loss: 0.27355077862739563\n",
      "Epoch 6: train loss: 0.39490535855293274\n",
      "Epoch 6: train loss: 0.37336304783821106\n",
      "Epoch 6: train loss: 0.3469502925872803\n",
      "Epoch 6: train loss: 0.4337223172187805\n",
      "Epoch 6: train loss: 0.23678091168403625\n",
      "Epoch 6: train loss: 0.25941649079322815\n",
      "Epoch 6: train loss: 0.3498058021068573\n",
      "Epoch 6: train loss: 0.2702062726020813\n",
      "Epoch 6: train loss: 0.3480093479156494\n",
      "Epoch 6: train loss: 0.41840657591819763\n",
      "Epoch 6: train loss: 0.3930566608905792\n",
      "Epoch 6: train loss: 0.4355795383453369\n",
      "Epoch 6: train loss: 0.3097817897796631\n",
      "Epoch 6: train loss: 0.4524385929107666\n",
      "Epoch 6: train loss: 0.4408959746360779\n",
      "Epoch 6: train loss: 0.4597107172012329\n",
      "Epoch 6: train loss: 0.35615965723991394\n",
      "Epoch 6: train loss: 0.30373474955558777\n",
      "Epoch 6: train loss: 0.47854921221733093\n",
      "Epoch 6: train loss: 0.61603844165802\n",
      "Epoch 6: train loss: 0.39393067359924316\n",
      "Epoch 6: train loss: 0.3095320463180542\n",
      "Epoch 6: train loss: 0.3907831609249115\n",
      "Epoch 6: train loss: 0.5236139297485352\n",
      "Epoch 6: train loss: 0.4143587350845337\n",
      "Epoch 6: train loss: 0.31019675731658936\n",
      "Epoch 6: train loss: 0.4897472560405731\n",
      "Epoch 7: train loss: 0.3939894139766693\n",
      "Epoch 7: train loss: 0.39296087622642517\n",
      "Epoch 7: train loss: 0.3629697561264038\n",
      "Epoch 7: train loss: 0.26265522837638855\n",
      "Epoch 7: train loss: 0.2617923617362976\n",
      "Epoch 7: train loss: 0.3514096140861511\n",
      "Epoch 7: train loss: 0.5006893873214722\n",
      "Epoch 7: train loss: 0.3249445855617523\n",
      "Epoch 7: train loss: 0.4765987694263458\n",
      "Epoch 7: train loss: 0.38526803255081177\n",
      "Epoch 7: train loss: 0.25802499055862427\n",
      "Epoch 7: train loss: 0.3519725501537323\n",
      "Epoch 7: train loss: 0.3347407281398773\n",
      "Epoch 7: train loss: 0.3937084972858429\n",
      "Epoch 7: train loss: 0.47865772247314453\n",
      "Epoch 7: train loss: 0.2664640247821808\n",
      "Epoch 7: train loss: 0.39694148302078247\n",
      "Epoch 7: train loss: 0.30477166175842285\n",
      "Epoch 7: train loss: 0.3475717604160309\n",
      "Epoch 7: train loss: 0.40564823150634766\n",
      "Epoch 7: train loss: 0.38288798928260803\n",
      "Epoch 7: train loss: 0.3044775426387787\n",
      "Epoch 7: train loss: 0.4768396317958832\n",
      "Epoch 7: train loss: 0.4865339994430542\n",
      "Epoch 7: train loss: 0.39037638902664185\n",
      "Epoch 7: train loss: 0.39016106724739075\n",
      "Epoch 7: train loss: 0.2305305451154709\n",
      "Epoch 7: train loss: 0.3091742992401123\n",
      "Epoch 7: train loss: 0.48019418120384216\n",
      "Epoch 7: train loss: 0.4050137400627136\n",
      "Epoch 7: train loss: 0.21942555904388428\n",
      "Epoch 7: train loss: 0.48239341378211975\n",
      "Epoch 7: train loss: 1.125000238418579\n",
      "Epoch 7: train loss: 0.17918705940246582\n",
      "Epoch 7: train loss: 0.28406351804733276\n",
      "Epoch 7: train loss: 0.392868310213089\n",
      "Epoch 7: train loss: 0.3598567843437195\n",
      "Epoch 7: train loss: 0.34709399938583374\n",
      "Epoch 7: train loss: 0.4335670471191406\n",
      "Epoch 7: train loss: 0.23481382429599762\n",
      "Epoch 7: train loss: 0.22864507138729095\n",
      "Epoch 7: train loss: 0.35306316614151\n",
      "Epoch 7: train loss: 0.27039825916290283\n",
      "Epoch 7: train loss: 0.35195255279541016\n",
      "Epoch 7: train loss: 0.4224203824996948\n",
      "Epoch 7: train loss: 0.39132505655288696\n",
      "Epoch 7: train loss: 0.43379995226860046\n",
      "Epoch 7: train loss: 0.30409371852874756\n",
      "Epoch 7: train loss: 0.44415372610092163\n",
      "Epoch 7: train loss: 0.4521016478538513\n",
      "Epoch 7: train loss: 0.45471200346946716\n",
      "Epoch 7: train loss: 0.36664631962776184\n",
      "Epoch 7: train loss: 0.30531731247901917\n",
      "Epoch 7: train loss: 0.4784679114818573\n",
      "Epoch 7: train loss: 0.6246309280395508\n",
      "Epoch 7: train loss: 0.39101940393447876\n",
      "Epoch 7: train loss: 0.30484721064567566\n",
      "Epoch 7: train loss: 0.3911040127277374\n",
      "Epoch 7: train loss: 0.5203427076339722\n",
      "Epoch 7: train loss: 0.41379743814468384\n",
      "Epoch 7: train loss: 0.3068768382072449\n",
      "Epoch 7: train loss: 0.48421239852905273\n",
      "Epoch 8: train loss: 0.3907976746559143\n",
      "Epoch 8: train loss: 0.39747339487075806\n",
      "Epoch 8: train loss: 0.37054818868637085\n",
      "Epoch 8: train loss: 0.2635529637336731\n",
      "Epoch 8: train loss: 0.26008132100105286\n",
      "Epoch 8: train loss: 0.36730846762657166\n",
      "Epoch 8: train loss: 0.4991791248321533\n",
      "Epoch 8: train loss: 0.3323866128921509\n",
      "Epoch 8: train loss: 0.476546972990036\n",
      "Epoch 8: train loss: 0.3959173560142517\n",
      "Epoch 8: train loss: 0.254668653011322\n",
      "Epoch 8: train loss: 0.35463961958885193\n",
      "Epoch 8: train loss: 0.34056365489959717\n",
      "Epoch 8: train loss: 0.3979189097881317\n",
      "Epoch 8: train loss: 0.45769545435905457\n",
      "Epoch 8: train loss: 0.2622699737548828\n",
      "Epoch 8: train loss: 0.40717244148254395\n",
      "Epoch 8: train loss: 0.29005730152130127\n",
      "Epoch 8: train loss: 0.3468974828720093\n",
      "Epoch 8: train loss: 0.40169623494148254\n",
      "Epoch 8: train loss: 0.3665846288204193\n",
      "Epoch 8: train loss: 0.35030919313430786\n",
      "Epoch 8: train loss: 0.4784666895866394\n",
      "Epoch 8: train loss: 0.4819459915161133\n",
      "Epoch 8: train loss: 0.4145069420337677\n",
      "Epoch 8: train loss: 0.39089497923851013\n",
      "Epoch 8: train loss: 0.21997004747390747\n",
      "Epoch 8: train loss: 0.30963999032974243\n",
      "Epoch 8: train loss: 0.477569043636322\n",
      "Epoch 8: train loss: 0.4029138684272766\n",
      "Epoch 8: train loss: 0.22520625591278076\n",
      "Epoch 8: train loss: 0.4771074950695038\n",
      "Epoch 8: train loss: 1.3434712886810303\n",
      "Epoch 8: train loss: 0.17583167552947998\n",
      "Epoch 8: train loss: 0.2816500663757324\n",
      "Epoch 8: train loss: 0.3935803771018982\n",
      "Epoch 8: train loss: 0.36394721269607544\n",
      "Epoch 8: train loss: 0.34726324677467346\n",
      "Epoch 8: train loss: 0.43993398547172546\n",
      "Epoch 8: train loss: 0.23299401998519897\n",
      "Epoch 8: train loss: 0.2321409434080124\n",
      "Epoch 8: train loss: 0.34912770986557007\n",
      "Epoch 8: train loss: 0.26640740036964417\n",
      "Epoch 8: train loss: 0.3507799804210663\n",
      "Epoch 8: train loss: 0.4091901183128357\n",
      "Epoch 8: train loss: 0.3945162296295166\n",
      "Epoch 8: train loss: 0.434516966342926\n",
      "Epoch 8: train loss: 0.30535122752189636\n",
      "Epoch 8: train loss: 0.445528119802475\n",
      "Epoch 8: train loss: 0.4393976032733917\n",
      "Epoch 8: train loss: 0.44649815559387207\n",
      "Epoch 8: train loss: 0.357928067445755\n",
      "Epoch 8: train loss: 0.3037721812725067\n",
      "Epoch 8: train loss: 0.47703903913497925\n",
      "Epoch 8: train loss: 0.6135995388031006\n",
      "Epoch 8: train loss: 0.3967425525188446\n",
      "Epoch 8: train loss: 0.32331931591033936\n",
      "Epoch 8: train loss: 0.39973029494285583\n",
      "Epoch 8: train loss: 0.5201797485351562\n",
      "Epoch 8: train loss: 0.3997141420841217\n",
      "Epoch 8: train loss: 0.3085598945617676\n",
      "Epoch 8: train loss: 0.4827553927898407\n",
      "Epoch 9: train loss: 0.3993837535381317\n",
      "Epoch 9: train loss: 0.393575519323349\n",
      "Epoch 9: train loss: 0.3555975556373596\n",
      "Epoch 9: train loss: 0.26223456859588623\n",
      "Epoch 9: train loss: 0.26036110520362854\n",
      "Epoch 9: train loss: 0.3512306213378906\n",
      "Epoch 9: train loss: 0.49587133526802063\n",
      "Epoch 9: train loss: 0.31433847546577454\n",
      "Epoch 9: train loss: 0.476550817489624\n",
      "Epoch 9: train loss: 0.392222136259079\n",
      "Epoch 9: train loss: 0.274655818939209\n",
      "Epoch 9: train loss: 0.3543255925178528\n",
      "Epoch 9: train loss: 0.33268123865127563\n",
      "Epoch 9: train loss: 0.39118316769599915\n",
      "Epoch 9: train loss: 0.4616714417934418\n",
      "Epoch 9: train loss: 0.26528164744377136\n",
      "Epoch 9: train loss: 0.4094567894935608\n",
      "Epoch 9: train loss: 0.2911761999130249\n",
      "Epoch 9: train loss: 0.35007959604263306\n",
      "Epoch 9: train loss: 0.39820605516433716\n",
      "Epoch 9: train loss: 0.371472030878067\n",
      "Epoch 9: train loss: 0.30355653166770935\n",
      "Epoch 9: train loss: 0.4886226952075958\n",
      "Epoch 9: train loss: 0.48952293395996094\n",
      "Epoch 9: train loss: 0.39280590415000916\n",
      "Epoch 9: train loss: 0.3899015486240387\n",
      "Epoch 9: train loss: 0.231923446059227\n",
      "Epoch 9: train loss: 0.31208568811416626\n",
      "Epoch 9: train loss: 0.47756484150886536\n",
      "Epoch 9: train loss: 0.3976931869983673\n",
      "Epoch 9: train loss: 0.22346365451812744\n",
      "Epoch 9: train loss: 0.48052337765693665\n",
      "Epoch 9: train loss: 0.9241757988929749\n",
      "Epoch 9: train loss: 0.17846472561359406\n",
      "Epoch 9: train loss: 0.26922470331192017\n",
      "Epoch 9: train loss: 0.39075568318367004\n",
      "Epoch 9: train loss: 0.3539816439151764\n",
      "Epoch 9: train loss: 0.3476221561431885\n",
      "Epoch 9: train loss: 0.43339309096336365\n",
      "Epoch 9: train loss: 0.2268679291009903\n",
      "Epoch 9: train loss: 0.2266155332326889\n",
      "Epoch 9: train loss: 0.3523014187812805\n",
      "Epoch 9: train loss: 0.2628477215766907\n",
      "Epoch 9: train loss: 0.3519236147403717\n",
      "Epoch 9: train loss: 0.4223118722438812\n",
      "Epoch 9: train loss: 0.3902839720249176\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: train loss: 0.43452560901641846\n",
      "Epoch 9: train loss: 0.3038891851902008\n",
      "Epoch 9: train loss: 0.45247015357017517\n",
      "Epoch 9: train loss: 0.43685755133628845\n",
      "Epoch 9: train loss: 0.47734981775283813\n",
      "Epoch 9: train loss: 0.3646235466003418\n",
      "Epoch 9: train loss: 0.3036497235298157\n",
      "Epoch 9: train loss: 0.4793425500392914\n",
      "Epoch 9: train loss: 0.6185654401779175\n",
      "Epoch 9: train loss: 0.39103493094444275\n",
      "Epoch 9: train loss: 0.33367082476615906\n",
      "Epoch 9: train loss: 0.3951336741447449\n",
      "Epoch 9: train loss: 0.5201401710510254\n",
      "Epoch 9: train loss: 0.40963098406791687\n",
      "Epoch 9: train loss: 0.3054956793785095\n",
      "Epoch 9: train loss: 0.4789217710494995\n",
      "Test loss  0.7468196749687195\n"
     ]
    }
   ],
   "source": [
    "epoch = 10\n",
    "model.train()\n",
    "train_loss = 0\n",
    "\n",
    "batch_size = 16\n",
    "batch_no = len(x_train) // batch_size\n",
    "\n",
    "for epoch in range(epoch):  \n",
    "    for i in range(batch_no):\n",
    "        optimizer.zero_grad()   \n",
    "        \n",
    "        start = i*batch_size\n",
    "        end = start+batch_size\n",
    "        \n",
    "        x_var = Variable(torch.FloatTensor(x_train[start:end]))\n",
    "        y_var = Variable(torch.FloatTensor(y_train[start:end])) \n",
    "        \n",
    "        y_pred = model(x_var)       \n",
    "        loss = criterion(y_pred.squeeze(), y_var)\n",
    "        print('Epoch {}: train loss: {}'.format(epoch, loss.item()))    \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "        lossVal = loss.item() *1\n",
    "    if train_loss < lossVal:\n",
    "        print(\"Validation loss decreased ({:6f} ===> {:6f}). Saving the model...\".format(train_loss,lossVal))\n",
    "        torch.save(model.state_dict(), \"model.pt\")\n",
    "        train_loss = lossVal \n",
    "        \n",
    "model.eval()\n",
    "y_pred_test = model(x_test)\n",
    "after_train = criterion(y_pred_test.squeeze(), y_test) \n",
    "print('Test loss ' , after_train.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (fc1): Linear(in_features=2, out_features=16, bias=True)\n",
      "  (fc2): Linear(in_features=16, out_features=1, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      "  (relu): ReLU()\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n",
      "Test loss before training 0.7468196749687195\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(model)\n",
    "model.eval()\n",
    "y_pred = model(x_test)\n",
    "before_train = criterion(y_pred.squeeze(), y_test)\n",
    "print('Test loss before training' , before_train.item())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
